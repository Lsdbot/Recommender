{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9c3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1ed340",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/raw/train_data/\"\n",
    "TEST_PATH = \"../data/raw/test_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5da0ce6",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 11:33:04 WARN Utils: Your hostname, sergey-G3-3579 resolves to a loopback address: 127.0.1.1; using 10.4.41.7 instead (on interface wlo1)\n",
      "23/07/29 11:33:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/29 11:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a862e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"parquet\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(TRAIN_PATH + \"train_data_0.pq\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98bf1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = df.columns\n",
    "ohe_columns.remove(\"id\")\n",
    "ohe_columns.remove(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91523816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(df, ohe_columns) -> set:\n",
    "    encoded_columns = set()\n",
    "\n",
    "    for col in ohe_columns:\n",
    "        pivot_table = (\n",
    "            df.groupBy('id')\n",
    "            .pivot(col)\n",
    "            .agg(F.count(col))\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        pivot_columns = pivot_table.columns\n",
    "        pivot_columns.remove(\"id\")\n",
    "\n",
    "        pivot_columns = list(map(lambda x: col + '_' + x, pivot_columns))\n",
    "\n",
    "        encoded_columns = encoded_columns.union(set(pivot_columns))\n",
    "    \n",
    "    return encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1de65cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_file(spark, file_path: str) -> set:\n",
    "    df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format(\"parquet\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(TRAIN_PATH + file_path)\n",
    "    )\n",
    "    \n",
    "    ohe_columns = df.columns\n",
    "    ohe_columns.remove(\"id\")\n",
    "    ohe_columns.remove(\"rn\")\n",
    "    \n",
    "    return get_columns(df, ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb6c634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = {'id', 'rn'}\n",
    "\n",
    "for file_path in os.listdir(TRAIN_PATH):\n",
    "    all_columns = all_columns.union(get_columns_from_file(spark, file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c646d8b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "319490a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/transformed_data/columns.yaml\", 'w') as file:\n",
    "    yaml.dump(list(all_columns), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce45c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cfe3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(TRAIN_PATH + \"train_data_0.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d008dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.id < 125000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6593291e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976676, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23e3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(\"category\")\n",
    "data[\"id\"] = data.id.astype(int)\n",
    "data[\"rn\"] = data.rn.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d212bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = pd.get_dummies(data, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ec5428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976676, 336)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5923f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/transformed_data/columns.yaml\", 'r') as file:\n",
    "    columns = yaml.load(file, yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f1b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f07541bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(encoded_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ec4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_columns = [x for x in columns if x not in encoded_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e860c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b8a3ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[col] = 0\n",
      "/tmp/ipykernel_52098/3002524520.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poo"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 14916 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in empty_columns:\n",
    "    encoded_data[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e3a7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976676, 421)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de91aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove(\"id\")\n",
    "columns.remove(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e021112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = encoded_data.groupby(\"id\")[columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd2a85ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pre_since_confirmed_12</th>\n",
       "      <th>pre_loans3060_8</th>\n",
       "      <th>pre_loans_credit_limit_17</th>\n",
       "      <th>pre_since_confirmed_1</th>\n",
       "      <th>enc_loans_account_holder_type_6</th>\n",
       "      <th>pre_loans5_6</th>\n",
       "      <th>is_zero_over2limit_0</th>\n",
       "      <th>pre_util_19</th>\n",
       "      <th>enc_paym_9_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pre_loans530_11</th>\n",
       "      <th>pre_maxover2limit_1</th>\n",
       "      <th>pclose_flag_1</th>\n",
       "      <th>enc_paym_0_0</th>\n",
       "      <th>enc_paym_22_1</th>\n",
       "      <th>is_zero_loans6090_1</th>\n",
       "      <th>enc_paym_8_1</th>\n",
       "      <th>pre_till_fclose_13</th>\n",
       "      <th>pre_over2limit_1</th>\n",
       "      <th>rn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 421 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pre_since_confirmed_12  pre_loans3060_8  pre_loans_credit_limit_17  \\\n",
       "0   0                       1                0                          0   \n",
       "1   1                       0                0                          0   \n",
       "2   2                       0                0                          0   \n",
       "3   3                       0                0                          1   \n",
       "4   4                       0                0                          0   \n",
       "\n",
       "   pre_since_confirmed_1  enc_loans_account_holder_type_6  pre_loans5_6  \\\n",
       "0                      1                                0            10   \n",
       "1                      0                                0            14   \n",
       "2                      0                                0             3   \n",
       "3                      1                                0            15   \n",
       "4                      0                                0             1   \n",
       "\n",
       "   is_zero_over2limit_0  pre_util_19  enc_paym_9_1  ...  pre_loans530_11  \\\n",
       "0                     0            0             0  ...                0   \n",
       "1                     0            0             0  ...                0   \n",
       "2                     0            0             0  ...                0   \n",
       "3                     0            0             0  ...                0   \n",
       "4                     0            0             0  ...                0   \n",
       "\n",
       "   pre_maxover2limit_1  pclose_flag_1  enc_paym_0_0  enc_paym_22_1  \\\n",
       "0                    0              1             0              0   \n",
       "1                    0              1             0              1   \n",
       "2                    0              2             0              1   \n",
       "3                    0              5             0              0   \n",
       "4                    0              1             0              0   \n",
       "\n",
       "   is_zero_loans6090_1  enc_paym_8_1  pre_till_fclose_13  pre_over2limit_1  rn  \n",
       "0                   10             0                   0                 0  10  \n",
       "1                   12             0                   2                 0  14  \n",
       "2                    2             0                   0                 0   3  \n",
       "3                   15             0                   0                 0  15  \n",
       "4                    1             0                   0                 0   1  \n",
       "\n",
       "[5 rows x 421 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "503d63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['rn'] = encoded_data.groupby(\"id\")['rn'].last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6431892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = aggregated_data.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c6cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.to_parquet(\"../data/transformed_data/train_data_0.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95149e1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b5830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
